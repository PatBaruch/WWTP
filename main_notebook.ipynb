{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0160410",
   "metadata": {},
   "source": [
    "# Reminder: Download the latest 'data' folder from Teams and replace yours before start working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc62edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6afa7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load all raw data files\n",
    "# We read multiple data sources including influent, oxygen sensors A and B, ammonium, nitrate, phosphate,\n",
    "# effluent, and weather data from various formats (parquet, excel, csv).\n",
    "df_influent = pd.read_parquet('data/raw-data/Influent_2023.parquet')\n",
    "df_oxygen_A = pd.read_parquet('data/raw-data/oxygen_a_2023.parquet')\n",
    "df_oxygen_B = pd.read_parquet('data/raw-data/oxygen_b_2023.parquet')\n",
    "df_ammonium = pd.read_parquet('data/raw-data/ammonium_2023.parquet')\n",
    "df_nitrite = pd.read_parquet('data/raw-data/nitrate_2023.parquet')\n",
    "df_phosphate = pd.read_parquet('data/raw-data/phosphate_2023.parquet')\n",
    "df_effluent = pd.read_excel('data/raw-data/effluent_2023.xlsx')\n",
    "df_weather = pd.read_csv('data/weather-data/weather_2023.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81afc57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the year of these dataframes are 2023, and see their columns names & records frenquency.\n",
    "\n",
    "# influent data - recorded every minute\n",
    "# df_influent.head()\n",
    "\n",
    "# oxygen data (A & B) - need to be merged (take the average) first - recorded every minute\n",
    "# df_oxygen_A.head()\n",
    "# df_oxygen_B.head()\n",
    "\n",
    "# chemical data (ammonium, nitrite, phosphate) - recorded every minute\n",
    "# df_ammonium.head()\n",
    "# df_nitrite.head()\n",
    "# df_phosphate.head()\n",
    "\n",
    "# effluent data - recorded every 15 minutes\n",
    "# df_effluent.head() 15min\n",
    "\n",
    "# weather data - recorded every hour\n",
    "# df_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0388e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            historianTagnummer hstWaarde     datumBeginMeting  \\\n",
      "40439  EDE_B121069901_K600.MTW     1.807  2023-10-29 01:59:00   \n",
      "40440  EDE_B121069901_K600.MTW     1.893  2023-10-29 01:59:00   \n",
      "40441  EDE_B121069901_K600.MTW     1.835  2023-10-29 02:00:00   \n",
      "40442  EDE_B121069901_K600.MTW     1.831  2023-10-29 02:00:00   \n",
      "40443  EDE_B121069901_K600.MTW     1.824  2023-10-29 02:01:00   \n",
      "40444  EDE_B121069901_K600.MTW     1.839  2023-10-29 02:01:00   \n",
      "40445  EDE_B121069901_K600.MTW     1.867  2023-10-29 02:02:00   \n",
      "40446  EDE_B121069901_K600.MTW     1.855  2023-10-29 02:02:00   \n",
      "40447  EDE_B121069901_K600.MTW     1.845  2023-10-29 02:03:00   \n",
      "40448  EDE_B121069901_K600.MTW     1.841  2023-10-29 02:03:00   \n",
      "40449  EDE_B121069901_K600.MTW     1.863  2023-10-29 02:04:00   \n",
      "40450  EDE_B121069901_K600.MTW     1.825  2023-10-29 02:04:00   \n",
      "40451  EDE_B121069901_K600.MTW     1.874  2023-10-29 02:05:00   \n",
      "40452  EDE_B121069901_K600.MTW     1.812  2023-10-29 02:05:00   \n",
      "40454  EDE_B121069901_K600.MTW     1.852  2023-10-29 02:06:00   \n",
      "40453  EDE_B121069901_K600.MTW     1.674  2023-10-29 02:06:00   \n",
      "40455  EDE_B121069901_K600.MTW     1.821  2023-10-29 02:07:00   \n",
      "40456  EDE_B121069901_K600.MTW     1.599  2023-10-29 02:07:00   \n",
      "40457  EDE_B121069901_K600.MTW     1.543  2023-10-29 02:08:00   \n",
      "40458  EDE_B121069901_K600.MTW     1.844  2023-10-29 02:08:00   \n",
      "\n",
      "          datumEindeMeting waardebewerkingsmethodeCode  \n",
      "40439  2023-10-29 02:00:00                              \n",
      "40440  2023-10-29 02:00:00                              \n",
      "40441  2023-10-29 02:01:00                              \n",
      "40442  2023-10-29 02:01:00                              \n",
      "40443  2023-10-29 02:02:00                              \n",
      "40444  2023-10-29 02:02:00                              \n",
      "40445  2023-10-29 02:03:00                              \n",
      "40446  2023-10-29 02:03:00                              \n",
      "40447  2023-10-29 02:04:00                              \n",
      "40448  2023-10-29 02:04:00                              \n",
      "40449  2023-10-29 02:05:00                              \n",
      "40450  2023-10-29 02:05:00                              \n",
      "40451  2023-10-29 02:06:00                              \n",
      "40452  2023-10-29 02:06:00                              \n",
      "40454  2023-10-29 02:07:00                              \n",
      "40453  2023-10-29 02:07:00                              \n",
      "40455  2023-10-29 02:08:00                              \n",
      "40456  2023-10-29 02:08:00                              \n",
      "40457  2023-10-29 02:09:00                              \n",
      "40458  2023-10-29 02:09:00                              \n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate timestamps in oxygen sensor A data to understand data quality issues\n",
    "duplicates = df_oxygen_A[df_oxygen_A.duplicated(subset='datumBeginMeting', keep=False)]\n",
    "print(duplicates.sort_values('datumBeginMeting').head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682c0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process data with duplicates and missing timestamps\n",
    "# This function:\n",
    "# - Handles duplicate timestamps by averaging their values\n",
    "# - Converts timestamps to Amsterdam timezone considering daylight saving time\n",
    "# - Fills in missing timestamps by reindexing and forward filling missing values\n",
    "\n",
    "def process_data_with_duplicates(df, time_col='datumBeginMeting', value_col='hstWaarde', freq='min'):\n",
    "    # Make a copy to avoid modifying the original DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Convert the time column to datetime format\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "    # Ensure the value column is numeric, coercing errors to NaN\n",
    "    df[value_col] = pd.to_numeric(df[value_col], errors='coerce')\n",
    "\n",
    "    # Find and handle duplicate timestamps by averaging their values\n",
    "    duplicates = df.duplicated(subset=time_col, keep=False).sum()\n",
    "    print(f\"Found {duplicates} duplicate timestamps\")\n",
    "    if duplicates > 0:\n",
    "        df = df.groupby(time_col, as_index=False)[value_col].mean()\n",
    "\n",
    "    # Set the time column as index\n",
    "    df.set_index(time_col, inplace=True)\n",
    "\n",
    "    # Assume original timestamps are UTC and convert to Amsterdam time zone (handles DST automatically)\n",
    "    df.index = df.index.tz_localize('UTC').tz_convert('Europe/Amsterdam')\n",
    "\n",
    "    # Generate a complete timestamp range with specified frequency in Amsterdam time zone\n",
    "    start = df.index.min()\n",
    "    end = df.index.max()\n",
    "    full_range = pd.date_range(start=start, end=end, freq=freq, tz='Europe/Amsterdam')\n",
    "\n",
    "    # Reindex the DataFrame to the full range, marking missing timestamps as NaN\n",
    "    df = df.reindex(full_range)\n",
    "\n",
    "    # Remove timezone information, keeping naive timestamps\n",
    "    df.index = df.index.tz_localize(None)\n",
    "\n",
    "    # Report missing timestamps before filling\n",
    "    missing_before = df[value_col].isna().sum()\n",
    "    print(f\"Initial missing timestamps count: {missing_before}\")\n",
    "    if missing_before > 0:\n",
    "        print(\"Example missing timestamps:\", df[df[value_col].isna()].index[:5])\n",
    "\n",
    "    # Forward fill missing values to maintain continuity\n",
    "    df.ffill(inplace=True)\n",
    "\n",
    "    # Report remaining missing timestamps after filling\n",
    "    missing_after = df[value_col].isna().sum()\n",
    "    print(f\"Missing timestamps after forward fill: {missing_after}\")\n",
    "\n",
    "    # Reset index to convert time back to a column\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={'index': time_col})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f59faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_oxygen_data(df_a, df_b):\n",
    "    # Process sensor A data\n",
    "    df_a_processed = process_data_with_duplicates(df_a, value_col='hstWaarde')\n",
    "    df_a_processed = df_a_processed.rename(columns={'hstWaarde': 'oxygen_a'})\n",
    "    \n",
    "    # Process sensor B data\n",
    "    df_b_processed = process_data_with_duplicates(df_b, value_col='hstWaarde')\n",
    "    df_b_processed = df_b_processed.rename(columns={'hstWaarde': 'oxygen_b'})\n",
    "    \n",
    "    # Merge sensor A and B data on timestamp\n",
    "    df_oxygen = pd.merge(df_a_processed, df_b_processed, on='datumBeginMeting', how='inner')\n",
    "    \n",
    "    # Calculate average oxygen value from both sensors\n",
    "    df_oxygen['oxygen_avg'] = (df_oxygen['oxygen_a'] + df_oxygen['oxygen_b']) / 2\n",
    "    \n",
    "    return df_oxygen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de4c111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing oxygen sensor A data:\n",
      "Found 120 duplicate timestamps\n",
      "Initial missing timestamps count: 611\n",
      "Example missing timestamps: DatetimeIndex(['2023-01-03 10:09:00', '2023-01-03 10:10:00',\n",
      "               '2023-01-03 10:11:00', '2023-01-03 10:12:00',\n",
      "               '2023-01-03 10:13:00'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Missing timestamps after forward fill: 0\n",
      "\n",
      "Processing oxygen sensor B data:\n",
      "Found 120 duplicate timestamps\n",
      "Initial missing timestamps count: 611\n",
      "Example missing timestamps: DatetimeIndex(['2023-01-03 10:09:00', '2023-01-03 10:10:00',\n",
      "               '2023-01-03 10:11:00', '2023-01-03 10:12:00',\n",
      "               '2023-01-03 10:13:00'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Missing timestamps after forward fill: 0\n"
     ]
    }
   ],
   "source": [
    "# Apply processing to oxygen A and B data, printing status along the way\n",
    "print(\"Processing oxygen sensor A data:\")\n",
    "df_oxygen_A_processed = process_data_with_duplicates(df_oxygen_A)\n",
    "print(\"\\nProcessing oxygen sensor B data:\")\n",
    "df_oxygen_B_processed = process_data_with_duplicates(df_oxygen_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35f1bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine processed oxygen sensor data and calculate average oxygen concentration\n",
    "a_clean = df_oxygen_A_processed.rename(columns={'hstWaarde': 'oxygen_a'})\n",
    "b_clean = df_oxygen_B_processed.rename(columns={'hstWaarde': 'oxygen_b'})\n",
    "df_oxygen = pd.merge(a_clean, b_clean, on='datumBeginMeting', how='inner')\n",
    "df_oxygen['oxygen_avg'] = (df_oxygen['oxygen_a'] + df_oxygen['oxygen_b']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99b8133a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing influent data:\n",
      "Found 120 duplicate timestamps\n",
      "Initial missing timestamps count: 421\n",
      "Example missing timestamps: DatetimeIndex(['2023-03-01 10:05:00', '2023-03-01 10:06:00',\n",
      "               '2023-03-01 10:07:00', '2023-03-01 10:08:00',\n",
      "               '2023-03-01 10:09:00'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Missing timestamps after forward fill: 0\n",
      "\n",
      "Processing ammonium data:\n",
      "Found 120 duplicate timestamps\n",
      "Initial missing timestamps count: 611\n",
      "Example missing timestamps: DatetimeIndex(['2023-01-03 10:09:00', '2023-01-03 10:10:00',\n",
      "               '2023-01-03 10:11:00', '2023-01-03 10:12:00',\n",
      "               '2023-01-03 10:13:00'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Missing timestamps after forward fill: 0\n",
      "\n",
      "Processing nitrate data:\n",
      "Found 120 duplicate timestamps\n",
      "Initial missing timestamps count: 611\n",
      "Example missing timestamps: DatetimeIndex(['2023-01-03 10:09:00', '2023-01-03 10:10:00',\n",
      "               '2023-01-03 10:11:00', '2023-01-03 10:12:00',\n",
      "               '2023-01-03 10:13:00'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Missing timestamps after forward fill: 0\n",
      "\n",
      "Processing phosphate data:\n",
      "Found 120 duplicate timestamps\n",
      "Initial missing timestamps count: 611\n",
      "Example missing timestamps: DatetimeIndex(['2023-01-03 10:09:00', '2023-01-03 10:10:00',\n",
      "               '2023-01-03 10:11:00', '2023-01-03 10:12:00',\n",
      "               '2023-01-03 10:13:00'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "Missing timestamps after forward fill: 0\n"
     ]
    }
   ],
   "source": [
    "# Process other water quality datasets: influent, ammonium, nitrate, and phosphate\n",
    "# We apply the same cleaning function to handle duplicates and missing timestamps\n",
    "print(\"\\nProcessing influent data:\")\n",
    "df_influent_processed = process_data_with_duplicates(df_influent)\n",
    "\n",
    "print(\"\\nProcessing ammonium data:\")\n",
    "df_ammonium_processed = process_data_with_duplicates(df_ammonium)\n",
    "\n",
    "print(\"\\nProcessing nitrate data:\")\n",
    "df_nitrate_processed = process_data_with_duplicates(df_nitrite)\n",
    "\n",
    "print(\"\\nProcessing phosphate data:\")\n",
    "df_phosphate_processed = process_data_with_duplicates(df_phosphate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "528c1118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all cleaned water quality data into a single DataFrame\n",
    "\n",
    "# Rename columns for clarity before merging on timestamp\n",
    "influent_clean = df_influent_processed.rename(columns={'hstWaarde': 'Influent'})\n",
    "oxygen_clean = df_oxygen[['datumBeginMeting', 'oxygen_avg']].rename(columns={'oxygen_avg': 'Oxygen'})\n",
    "ammonium_clean = df_ammonium_processed.rename(columns={'hstWaarde': 'Ammonium'})\n",
    "nitrate_clean = df_nitrate_processed.rename(columns={'hstWaarde': 'Nitrate'})\n",
    "phosphate_clean = df_phosphate_processed.rename(columns={'hstWaarde': 'Phosphate'})\n",
    "\n",
    "# Merge all water quality datasets on the common timestamp column\n",
    "df_water = influent_clean.merge(oxygen_clean, on='datumBeginMeting', how='inner') \\\n",
    "                        .merge(ammonium_clean, on='datumBeginMeting', how='inner') \\\n",
    "                        .merge(nitrate_clean, on='datumBeginMeting', how='inner') \\\n",
    "                        .merge(phosphate_clean, on='datumBeginMeting', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea08ab",
   "metadata": {},
   "source": [
    "After reviewing the weather dataset, I found that the Rain column only contains binary values: 0 indicates no rain, and 1 indicates rain. So, I decided to use the PrecipitationAmount column this time, which records the actual hourly precipitation amount in millimeters (mm). Additionally, the temperature values in the dataset have already been adjusted by dividing them by 10, so the unit is now degrees Celsius (°C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f343d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process weather data\n",
    "# Convert timestamp to Amsterdam timezone and rename columns for consistency\n",
    "df_weather['Timestamp'] = pd.to_datetime(df_weather['Timestamp']).dt.tz_localize('UTC').dt.tz_convert('Europe/Amsterdam')\n",
    "df_weather = df_weather.rename(columns={'Timestamp': 'date', 'PrecipitationAmount': 'Rainfall'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "493c12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge water quality and weather datasets on datetime\n",
    "\n",
    "# Remove timezone info for both datasets to ensure proper merging\n",
    "df_water['date'] = pd.to_datetime(df_water['datumBeginMeting']).dt.tz_localize(None)\n",
    "df_weather['date'] = pd.to_datetime(df_weather['date']).dt.tz_localize(None)\n",
    "\n",
    "# Merge on 'date' column\n",
    "df_main = pd.merge(df_water, df_weather, on='date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fece5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/nmdc67354c9f77107p22bnd00000gn/T/ipykernel_62882/3977377120.py:4: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df_main = df_main.resample('H').mean().reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Resample the merged data to hourly frequency by taking the mean\n",
    "# This reduces data granularity from minutes to hours, which smooths noise and aligns with analysis needs\n",
    "df_main.set_index('date', inplace=True)\n",
    "df_main = df_main.resample('H').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62e3cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final missing value check:\n",
      "date                0\n",
      "datumBeginMeting    1\n",
      "Influent            1\n",
      "Oxygen              1\n",
      "Ammonium            1\n",
      "Nitrate             1\n",
      "Phosphate           1\n",
      "Rainfall            1\n",
      "Temperature         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Final check for missing values in the combined dataset\n",
    "print(\"\\nFinal missing value check:\")\n",
    "print(df_main.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "970e8fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    date datumBeginMeting  Influent  Oxygen  Ammonium  \\\n",
      "2017 2023-03-26 02:00:00              NaT       NaN     NaN       NaN   \n",
      "\n",
      "      Nitrate  Phosphate  Rainfall  Temperature  \n",
      "2017      NaN        NaN       NaN          NaN  \n"
     ]
    }
   ],
   "source": [
    "# Display rows with missing values for review\n",
    "missing_hours = df_main[df_main.isna().any(axis=1)]\n",
    "print(missing_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ce0fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill remaining missing values using forward fill to maintain data continuity\n",
    "df_main.ffill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3edd6887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8760 entries, 0 to 8759\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   date              8760 non-null   datetime64[ns]\n",
      " 1   datumBeginMeting  8760 non-null   datetime64[ns]\n",
      " 2   Influent          8760 non-null   float64       \n",
      " 3   Oxygen            8760 non-null   float64       \n",
      " 4   Ammonium          8760 non-null   float64       \n",
      " 5   Nitrate           8760 non-null   float64       \n",
      " 6   Phosphate         8760 non-null   float64       \n",
      " 7   Rainfall          8760 non-null   float64       \n",
      " 8   Temperature       8760 non-null   float64       \n",
      "dtypes: datetime64[ns](2), float64(7)\n",
      "memory usage: 616.1 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>datumBeginMeting</th>\n",
       "      <th>Influent</th>\n",
       "      <th>Oxygen</th>\n",
       "      <th>Ammonium</th>\n",
       "      <th>Nitrate</th>\n",
       "      <th>Phosphate</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "      <td>3330.199</td>\n",
       "      <td>1.2200</td>\n",
       "      <td>1.310</td>\n",
       "      <td>4.730</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 02:00:00</td>\n",
       "      <td>2023-01-01 02:00:00</td>\n",
       "      <td>2916.007</td>\n",
       "      <td>0.0540</td>\n",
       "      <td>1.119</td>\n",
       "      <td>3.616</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 03:00:00</td>\n",
       "      <td>2023-01-01 03:00:00</td>\n",
       "      <td>2573.291</td>\n",
       "      <td>1.5320</td>\n",
       "      <td>2.481</td>\n",
       "      <td>3.239</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "      <td>2624.023</td>\n",
       "      <td>1.7755</td>\n",
       "      <td>1.102</td>\n",
       "      <td>3.696</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 05:00:00</td>\n",
       "      <td>2023-01-01 05:00:00</td>\n",
       "      <td>1426.117</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>1.485</td>\n",
       "      <td>2.513</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date    datumBeginMeting  Influent  Oxygen  Ammonium  \\\n",
       "0 2023-01-01 01:00:00 2023-01-01 01:00:00  3330.199  1.2200     1.310   \n",
       "1 2023-01-01 02:00:00 2023-01-01 02:00:00  2916.007  0.0540     1.119   \n",
       "2 2023-01-01 03:00:00 2023-01-01 03:00:00  2573.291  1.5320     2.481   \n",
       "3 2023-01-01 04:00:00 2023-01-01 04:00:00  2624.023  1.7755     1.102   \n",
       "4 2023-01-01 05:00:00 2023-01-01 05:00:00  1426.117  0.1885     1.485   \n",
       "\n",
       "   Nitrate  Phosphate  Rainfall  Temperature  \n",
       "0    4.730      0.001       0.0         15.3  \n",
       "1    3.616      0.010       0.0         14.6  \n",
       "2    3.239      0.367       0.0         15.0  \n",
       "3    3.696      0.012       0.0         14.7  \n",
       "4    2.513      0.281       0.0         14.1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the cleaned and combined dataset to CSV for future use\n",
    "df_main.to_csv(\"data/main_2023.csv\", index=False)\n",
    "\n",
    "# Display basic info and first few rows to confirm successful processing\n",
    "df_main.info()\n",
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5302eeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min-Max scaling completed and saved!\n"
     ]
    }
   ],
   "source": [
    "# Normalize all feature columns using Min-Max scaling\n",
    "\n",
    "# only select numeric columns for scaling\n",
    "df_features = df_main.select_dtypes(include=['number'])\n",
    "\n",
    "# Initialize Min-Max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit scaler and transform the features\n",
    "df_scaled_values = scaler.fit_transform(df_features)\n",
    "\n",
    "# Create a DataFrame with scaled values and original column names\n",
    "df_minmax_scaled = pd.DataFrame(df_scaled_values, columns=df_features.columns)\n",
    "\n",
    "# Add back the date column\n",
    "df_minmax_scaled['date'] = df_main['date']\n",
    "\n",
    "# Reorder columns to keep date as the first column\n",
    "df_minmax_scaled = df_minmax_scaled[['date'] + df_features.columns.tolist()]\n",
    "\n",
    "# Save the scaled dataset to CSV\n",
    "df_minmax_scaled.to_csv(\"data/minmax_scaled_main_2023.csv\", index=False)\n",
    "\n",
    "print(\"\\nMin-Max scaling completed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f5250cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardization scaling completed and saved!\n"
     ]
    }
   ],
   "source": [
    "# only select numeric columns for standardization\n",
    "df_features = df_main.select_dtypes(include=['number'])\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply standardization\n",
    "df_standartization_scaled_values = scaler.fit_transform(df_features)\n",
    "\n",
    "# Create a DataFrame with standardized values\n",
    "df_standartization_scaled = pd.DataFrame(df_standartization_scaled_values, columns=df_features.columns)\n",
    "\n",
    "# Add the date column back\n",
    "df_standartization_scaled['date'] = df_main['date']\n",
    "\n",
    "# Reorder columns to keep date first\n",
    "df_standartization_scaled = df_standartization_scaled[['date'] + df_features.columns.tolist()]\n",
    "\n",
    "# Save the standardized data to CSV\n",
    "df_standartization_scaled.to_csv(\"data/standard_scaled_main_2023.csv\", index=False)\n",
    "\n",
    "print(\"\\nStandardization scaling completed and saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
